{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"| Model                            | train_loss | val_loss | test_loss | version |\n|----------------------------------|------------|----------|-----------|---------|\n| Initial                          |     0.1155 |   0.4484 |    4.7467 |     V12 |\n| Cross Feature                    |     0.2275 |   0.8123 |    4.4129 |     V13 |\n| tsfresh                          |     0.2499 |   0.9606 |    4.4129 |     V14 |\n| Attention                        |     0.1141 |   0.4449 |    6.1804 |     V15 |\n| CNN+LSTM                         |     0.2341 |   0.8626 |    5.3613 |     V16 |\n| CNN                              |     0.1605 |   0.6452 |    4.4129 |     V26 |\n| LSTM Fusion                      |     0.2510 |   0.9487 |    6.8452 |     V17 |\n| Initial (Bayesian Optimization)  |     0.1255 |   0.5140 |    4.4526 |     V20 |\n| Initial (Short-term + Long-term) |     0.2261 |   0.8831 |    4.6863 |     V22 |","metadata":{}},{"cell_type":"code","source":"# Read data and aggregate into monthly data\n\nimport pandas as pd\n\ntrain_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\n# columns: date, date_block_num, shop_id, item_id, item_price, item_cnt_day\ntrain_monthly_df = train_df.groupby(['date_block_num','shop_id','item_id']).agg(sale_cnt_month=('item_cnt_day','sum'),price_month=('item_price','mean')).reset_index()\ntrain_monthly_df['shop_item'] = train_monthly_df['shop_id'].astype('string') + '-' + train_monthly_df['item_id'].astype('string')\nshop_item_list = pd.unique(train_monthly_df['shop_item'])\nshop_list = pd.unique(train_monthly_df['shop_id'])\nitem_list = pd.unique(train_monthly_df['item_id'])\n\nprint(\"count of shop_item:\",len(shop_item_list))\nprint('------')\nprint('count of shop_id:', len(shop_list))\nprint('min shop_id:', min(shop_list))\nprint('max shop id:', max(shop_list))\nprint('------')\nprint('count item_id:', len(item_list))\nprint('min item_id:', min(item_list))\nprint('max item id:', max(item_list))\ntrain_monthly_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the \"sale_cnt sequences\" of each \"shop_item\"\n## get (data_seq_dict)\n\nimport numpy as np\n\ndata_seq_dict = {}\nfor shop_item in shop_item_list:\n    data_seq_dict[shop_item] = [0.0 for i in range(0,34)]\nfor _, row in train_monthly_df.iterrows():\n    data_seq_dict[row['shop_item']][int(row['date_block_num'])] = row['sale_cnt_month']\n    \nprint(len(data_seq_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the \"normalized sale_cnt sequences\" and ”mean“ of each \"shop_item\"\n## get (data_norm_seq_dict), as well as (data_norm_seq_mean_dict) and (data_norm_seq_stddev_dict)\n\nimport statistics\n\ndata_norm_seq_dict = {}\ndata_norm_seq_mean_dict = {}\ndata_norm_seq_stddev_dict = {}\nfor k,v in data_seq_dict.items():\n    mean = statistics.mean(v)\n    data_norm_seq_mean_dict[k] = mean\n    stddev = statistics.stdev(v)\n    data_norm_seq_stddev_dict[k] = stddev\n    if stddev != 0.0:\n        data_norm_seq_dict[k] = [(x-mean)/stddev for x in v]\n    else:\n        data_norm_seq_dict[k] = [0.0 for x in v]\n\nprint(len(data_norm_seq_dict))\nprint(len(data_norm_seq_mean_dict))\nprint(len(data_norm_seq_stddev_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the \"last price\" of each \"shop_item\" as the future price\n## shopitem_price_dict\n\nimport random\n\nshopitem_price_dict = {}  # {shop_item: [avg_price, current_max_date_block_num, cnt]}\n# record price and max date_block_num of each shop_item\nfor _, row in train_monthly_df.iterrows():\n    if row['shop_item'] not in shopitem_price_dict.keys():\n        shopitem_price_dict[row['shop_item']] = [row['price_month'], row['date_block_num'], 1]\n    else:\n        if row['date_block_num'] > shopitem_price_dict[row['shop_item']][1]:\n            shopitem_price_dict[row['shop_item']] = [row['price_month'], row['date_block_num'], 1]\n        if row['date_block_num'] == shopitem_price_dict[row['shop_item']][1]:\n            entry = shopitem_price_dict[row['shop_item']]\n            count = entry[2] + 1\n            price = (entry[0] * entry[2] + row['price_month']) / count\n            shopitem_price_dict[row['shop_item']] = [price, row['date_block_num'], count]\n# remove date_block_num and count in the dict to only keep [shop_item, price_month]\nfor k,v in shopitem_price_dict.items():\n    shopitem_price_dict[k] = v[0]\n\n# UT code: printout a random sample to check\nprint(len(shopitem_price_dict))\nidx = random.randint(0,len(shop_item_list))\nshop_item = shop_item_list[idx]\nprint(\"The last price of shop-item\", shop_item, \":\", shopitem_price_dict[shop_item])\nshop_item = shop_item.split('-')\ntrain_monthly_df[(train_monthly_df['shop_id']==int(shop_item[0])) &\n                 (train_monthly_df['item_id']==int(shop_item[1])) &\n                 (train_monthly_df['price_month']>0.0)].sort_values(by=['date_block_num'],ascending=[False])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the avg \"sale_cnt sequences\" of each \"item\" as default\n# In case the shop_item in test dataset does not appear in training dataset, \n# we use the \"item avg sale_cnt sequence\" as default.\n## get (item_seq_dict)\n\nitem_seq_dict = {}\nitem_seq_num_dict = {}\n# make sum\nfor k, v in data_seq_dict.items():\n    item = k.split('-')[1]\n    if item not in item_seq_dict.keys():\n        item_seq_dict[item] = v\n        item_seq_num_dict[item] = 1\n    else:\n        item_seq_dict[item] = [i+j for i, j in zip(item_seq_dict[item], v)]\n        item_seq_num_dict[item] += 1\n# make average\nfor item in item_seq_dict.keys():\n    item_seq_dict[item] = [element / item_seq_num_dict[item] for element in item_seq_dict[item]]\n\nprint(len(item_seq_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the \"normalized sale_cnt sequences\" and \"mean\" of each \"item\"\n## get (item_norm_seq_dict), as well as (item_norm_seq_mean_dict) and (item_norm_seq_stddev_dict)\n\nimport statistics\n\nitem_norm_seq_dict = {}\nitem_norm_seq_mean_dict = {}\nitem_norm_seq_stddev_dict = {}\nfor k,v in item_seq_dict.items():\n    mean = statistics.mean(v)\n    item_norm_seq_mean_dict[k] = mean\n    stddev = statistics.stdev(v)\n    item_norm_seq_stddev_dict[k] = stddev\n    if stddev != 0:\n        item_norm_seq_dict[k] = [(x-mean)/stddev for x in v]\n    else:\n        item_norm_seq_dict[k] = [0.0 for x in v]\n\nprint(len(item_norm_seq_dict))\nprint(len(item_norm_seq_mean_dict))\nprint(len(item_norm_seq_stddev_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the \"last price\" of each \"item\" as the future price\n## get (item_price_dict)\n\nitem_price_dict = {}  # {item_id: [avg_price, current_max_date_block_num, cnt]}\n# record prices and max date_block_num of each item\nfor _, row in train_monthly_df.iterrows():\n    item_id = str(row['item_id'])\n    if item_id not in item_price_dict.keys():\n        item_price_dict[item_id] = [row['price_month'], row['date_block_num'], 1]\n    else:\n        if row['date_block_num'] > item_price_dict[item_id][1]:\n            item_price_dict[item_id] = [row['price_month'], row['date_block_num'], 1]\n        if row['date_block_num'] == item_price_dict[item_id][1]:\n            entry = item_price_dict[item_id]\n            count = entry[2] + 1\n            price = (entry[0] * entry[2] + row['price_month']) / count\n            item_price_dict[item_id] = [price, row['date_block_num'], count]\n# remove date_block_num and count in the dict to only keep [item_id, price_month]\nfor k,v in item_price_dict.items():\n    item_price_dict[k] = v[0]\n    \n# UT code: printout a random sample to check\nprint(len(item_price_dict))\nidx = random.randint(0,len(item_list))\nitem_id = str(item_list[idx])\nprint(\"The last price of item\", item_id, \":\", item_price_dict[item_id])\ntmp_df = train_monthly_df[(train_monthly_df['item_id']==int(item_id))][['date_block_num', 'price_month']]\ntmp_df = tmp_df[tmp_df['date_block_num']==tmp_df['date_block_num'].max()][['price_month']].mean()\nprint(tmp_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # get the avg \"sale_cnt sequences\" of each \"shop\" as default\n# # Just for the usage in tsfresh, because the cardinality of shop_item or item_id is too large\n# ## get (shop_seq_dict)\n\n# shop_seq_dict = {}\n# shop_seq_num_dict = {}\n# # make sum\n# for k, v in data_seq_dict.items():\n#     shop = k.split('-')[0]\n#     if shop not in shop_seq_dict.keys():\n#         shop_seq_dict[shop] = v\n#         shop_seq_num_dict[shop] = 1\n#     else:\n#         shop_seq_dict[shop] = [i+j for i, j in zip(shop_seq_dict[shop], v)]\n#         shop_seq_num_dict[shop] += 1\n# # make average\n# for shop in shop_seq_dict.keys():\n#     shop_seq_dict[shop] = [element / shop_seq_num_dict[shop] for element in shop_seq_dict[shop]]\n\n# print(len(shop_seq_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # get the \"normalized sale_cnt sequences\" and \"mean\" of each \"shop\"\n# # Just for the usage in tsfresh, because the cardinality of shop_item or item_id is too large\n# ## get (shop_norm_seq_dict), as well as (shop_norm_seq_mean_dict) and (shop_norm_seq_stddev_dict)\n\n# import statistics\n\n# shop_norm_seq_dict = {}\n# shop_norm_seq_mean_dict = {}\n# shop_norm_seq_stddev_dict = {}\n# for k,v in shop_seq_dict.items():\n#     mean = statistics.mean(v)\n#     shop_norm_seq_mean_dict[k] = mean\n#     stddev = statistics.stdev(v)\n#     shop_norm_seq_stddev_dict[k] = stddev\n#     if stddev != 0:\n#         shop_norm_seq_dict[k] = [(x-mean)/stddev for x in v]\n#     else:\n#         shop_norm_seq_dict[k] = [0.0 for x in v]\n\n# print(len(shop_norm_seq_dict))\n# print(len(shop_norm_seq_mean_dict))\n# print(len(shop_norm_seq_stddev_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # get the \"last price\" of each \"shop\" as the future price\n# # Just for the usage in tsfresh, because the cardinality of shop_item or item_id is too large\n# ## get (shop_price_dict)\n\n# shop_price_dict = {}  # {shop_id: [avg_price, current_max_date_block_num, cnt]}\n# # record prices and max date_block_num of each shop\n# for _, row in train_monthly_df.iterrows():\n#     shop_id = str(row['shop_id'])\n#     if shop_id not in shop_price_dict.keys():\n#         shop_price_dict[shop_id] = [row['price_month'], row['date_block_num'], 1]\n#     else:\n#         if row['date_block_num'] > shop_price_dict[shop_id][1]:\n#             shop_price_dict[shop_id] = [row['price_month'], row['date_block_num'], 1]\n#         if row['date_block_num'] == shop_price_dict[shop_id][1]:\n#             entry = shop_price_dict[shop_id]\n#             count = entry[2] + 1\n#             price = (entry[0] * entry[2] + row['price_month']) / count\n#             shop_price_dict[shop_id] = [price, row['date_block_num'], count]\n# # remove date_block_num and count in the dict to only keep [shop_id, price_month]\n# for k,v in shop_price_dict.items():\n#     shop_price_dict[k] = v[0]\n    \n# # UT code: printout a random sample to check\n# print(len(shop_price_dict))\n# idx = random.randint(0,len(shop_list))\n# shop_id = str(shop_list[idx])\n# print(\"The last price of shop\", shop_id, \":\", shop_price_dict[shop_id])\n# tmp_df = train_monthly_df[(train_monthly_df['shop_id']==int(shop_id))][['date_block_num', 'price_month']]\n# tmp_df = tmp_df[tmp_df['date_block_num']==tmp_df['date_block_num'].max()][['price_month']].mean()\n# print(tmp_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot curve\n\nimport matplotlib.pyplot as plt\n\nrandom_idx_array = np.arange(len(shop_item_list))\nnp.random.shuffle(random_idx_array)\nrow_random = np.array(list(data_seq_dict.values()))[random_idx_array][0:100,:]\nfor seq in row_random:\n    plt.plot(seq)\nplt.xlabel('time step')\nplt.ylabel('sale cnt')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Preprocess\n\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import backend as K\n\n\n# parameter\nITEM_VOCAB_LEN = 1000\n\n# Scaler of price\nprice_scaler = MinMaxScaler(feature_range=(0,1))\nprice_scaler.fit([[i] for i in train_monthly_df['price_month'].tolist()])\n\n# Split X and Y\nX = []\nY = []\nsample_weight = []\nfor k,v in data_norm_seq_dict.items():\n    shop_item = k\n    static_vec = np.array([int(i) for i in k.split('-')])\n    static_vec[1] = static_vec[1] % ITEM_VOCAB_LEN\n    price_vec = price_scaler.transform([[shopitem_price_dict[k]]]).reshape((-1,))\n    static_vec = np.concatenate([static_vec, price_vec])\n    seq_vec = np.array(v).reshape((-1,1))  # (steps, 1)\n    X.append([shop_item, static_vec, seq_vec[:-1,:]])\n    Y.append(seq_vec[-1,:])\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.3)\nX_train_static = np.array([x[1] for x in X_train])\nX_train_seq = np.array([x[2] for x in X_train])\nY_train = np.array(Y_train)\nX_val_static = np.array([x[1] for x in X_val])\nX_val_seq = np.array([x[2] for x in X_val])\nY_val = np.array(Y_val)\n\n# Global Parameters\nSTEPS = X_train_seq.shape[1]\n\n# generate sample_weight, take mean as sample weight\nfor entry in X_train:\n    shop_item = entry[0]\n    sample_weight.append(data_norm_seq_mean_dict[shop_item])\nsample_weight = np.array(sample_weight)\n\nprint(\"Training Dataset:\")\nprint(\"\\nX_train_static:\")\nprint(X_train_static.shape)\nprint(X_train_static)\nprint(\"\\nX_train_seq:\")\nprint(X_train_seq.shape)\nprint(X_train_seq)\nprint(\"\\nsample_weight:\")\nprint(sample_weight.shape)\nprint(sample_weight)\nprint(\"\\nY_train:\")\nprint(Y_train.shape)\nprint(Y_train)\nprint(\"\\nValidation Dataset:\")\nprint(\"\\nX_val_static:\")\nprint(X_val_static.shape)\nprint(X_val_static)\nprint(\"\\nX_val_seq:\")\nprint(X_val_seq.shape)\nprint(X_val_seq)\nprint(\"\\nY_val:\")\nprint(Y_val.shape)\nprint(Y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initial Model, Wide&Deep\n\n# # Model Parameters\n# STEPS = X_train_seq.shape[1]\n\n# import os\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# from math import log\n# import tensorflow as tf\n# import tensorflow_probability as tfp\n# from tensorflow import feature_column\n# from tensorflow.nn import relu\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras import Model\n# from tensorflow.keras import Input\n# from tensorflow.keras.layers import Dense, Lambda, Concatenate, Reshape, Embedding, Conv1D, Add, Flatten, LSTM, Activation, BatchNormalization\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.metrics import mean_squared_error\n\n# # Loss\n# def root_mean_squared_error(y_true, y_pred):\n#     return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n# # Model\n# # Wide Component\n# input_static_vec = Input(name='input_static', shape = (3,))\n\n# shop_vec = Lambda(lambda x: tf.slice(x, (0,0), (-1,1)))(input_static_vec)\n# shop_emb_vec = Embedding(input_dim=60, output_dim=4, input_length=(1,))(shop_vec)\n# shop_emb_vec = Flatten()(shop_emb_vec)\n# item_vec = Lambda(lambda x: tf.slice(x, (0,1), (-1,1)))(input_static_vec)\n# item_emb_vec = Embedding(input_dim=ITEM_VOCAB_LEN, output_dim=4, input_length=(1,))(item_vec)\n# item_emb_vec = Flatten()(item_emb_vec)\n# price_vec = Lambda(lambda x: tf.slice(x, (0,2), (-1,1)))(input_static_vec)\n# wide_vec = Concatenate(axis=-1, name='Concat_wide')([shop_emb_vec, item_emb_vec, price_vec])\n\n# # Deep Component\n# input_seq_vec = Input(name='input_seq', shape = (STEPS,1))\n\n# deep_vec = LSTM(units=4, name='LSTM')(input_seq_vec)\n\n# # Fusion Layer\n# output_vec = Concatenate(axis=-1, name='Concat_fusion')([wide_vec, deep_vec])\n# output_vec = Dense(units=1, activation='relu')(output_vec)\n\n# model = Model(inputs=[input_static_vec,input_seq_vec], outputs=[output_vec])\n# model.compile(loss=root_mean_squared_error, optimizer=Adam(learning_rate=0.05))\n# model.summary()\n\n# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001,\n#                                               verbose=1, mode='min', patience=3,\n#                                               restore_best_weights=True)]\n\n# history = model.fit(x={'input_static': X_train_static, 'input_seq': X_train_seq}, \n#                     y=Y_train,\n#                     sample_weight=sample_weight,\n#                     validation_data=({'input_static':X_val_static,'input_seq':X_val_seq}, Y_val), \n#                     epochs=100, \n#                     batch_size=1024*1024,\n#                     callbacks=callbacks)\n\n# # Original Model, loss: 0.1155 - val_loss: 0.4484 - test_loss: 4.7467 (V12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Cross Feature Model\n\n# # Model Parameters\n# STEPS = X_train_seq.shape[1]\n\n# import os\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# from math import log\n# import tensorflow as tf\n# import tensorflow_probability as tfp\n# from tensorflow import feature_column\n# from tensorflow.nn import relu\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras import Model\n# from tensorflow.keras import Input\n# from tensorflow.keras.layers import Dense, Lambda, Concatenate, Reshape, Embedding, Conv1D, Add, Flatten, LSTM, Activation, BatchNormalization, Multiply\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.metrics import mean_squared_error\n\n# # Loss\n# def root_mean_squared_error(y_true, y_pred):\n#     return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n# # Model\n# # Wide Component\n# input_static_vec = Input(name='input_static', shape = (3,))\n\n# shop_vec = Lambda(lambda x: tf.slice(x, (0,0), (-1,1)))(input_static_vec)\n# shop_emb_vec = Embedding(input_dim=60, output_dim=4, input_length=(1,))(shop_vec)\n# shop_emb_vec = Flatten()(shop_emb_vec)\n# item_vec = Lambda(lambda x: tf.slice(x, (0,1), (-1,1)))(input_static_vec)\n# item_emb_vec = Embedding(input_dim=ITEM_VOCAB_LEN, output_dim=4, input_length=(1,))(item_vec)\n# item_emb_vec = Flatten()(item_emb_vec)\n# price_vec = Lambda(lambda x: tf.slice(x, (0,2), (-1,1)))(input_static_vec)\n# wide_vec = Concatenate(axis=-1, name='Concat_wide')([shop_emb_vec, item_emb_vec, price_vec])\n\n# # Cross Feature\n# shop_item_vec = Multiply()([shop_emb_vec, item_emb_vec])\n# item_price_vec = Multiply()([item_emb_vec, price_vec])\n\n# # Deep Component\n# input_seq_vec = Input(name='input_seq', shape = (STEPS,1))\n\n# deep_vec = LSTM(units=4, name='LSTM')(input_seq_vec)\n\n# # Fusion Layer\n# output_vec = Concatenate(axis=-1, name='Concat_fusion')([wide_vec, shop_item_vec, item_price_vec, deep_vec])\n# output_vec = Dense(units=1, activation='relu')(output_vec)\n\n# model = Model(inputs=[input_static_vec,input_seq_vec], outputs=[output_vec])\n# model.compile(loss=root_mean_squared_error, optimizer=Adam(learning_rate=0.05))\n# model.summary()\n\n# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001,\n#                                               verbose=1, mode='min', patience=3,\n#                                               restore_best_weights=True)]\n\n# history = model.fit(x={'input_static': X_train_static, 'input_seq': X_train_seq}, \n#                     y=Y_train,\n#                     sample_weight=sample_weight,\n#                     validation_data=({'input_static':X_val_static,'input_seq':X_val_seq}, Y_val), \n#                     epochs=100, \n#                     batch_size=1024*1024,\n#                     callbacks=callbacks)\n\n# # Cross Feature Model - loss: 0.2275 - val_loss: 0.8123 - test_loss: 4.41291 (V13)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # tsfresh Model\n\n# # generate the tsfresh feature of shop (shop_item cardinality too large)\n# # Ref: https://tsfresh.readthedocs.io/en/latest/\n\n\n# ### DATA PREPROCESS ###\n\n# from tsfresh import extract_relevant_features\n\n# # preprocess shop dataframe (take shop_id as example because cardinality of shop_item or item_id is too large)\n# X_shop = []\n# Y_shop = []\n# for k,v in shop_norm_seq_dict.items():\n#     shop_id = k\n#     static_vec = np.array([int(k)])\n#     price_vec = price_scaler.transform([[shop_price_dict[k]]]).reshape((-1,))\n#     static_vec = np.concatenate([static_vec, price_vec])\n#     seq_vec = np.array(v).reshape((-1,1))  # (steps, 1)\n#     X_shop.append([shop_id, static_vec, seq_vec[:-1,:]])\n#     Y_shop.append(seq_vec[-1,:])\n\n    \n# # generate tsfresh x and y\n# tsfresh_x_df = pd.DataFrame(columns=['shop_id', 'date_block_num', 'sale_cnt_month'])\n# tsfresh_y_df = pd.DataFrame(columns=['shop_id', 'sale_cnt_month'])\n# for idx in range(0,len(X_shop)):\n#     shop_id = X_shop[idx][0]\n#     seq_array = np.squeeze(X_shop[idx][2])  # (33,)\n#     for step in range(0,len(seq_array)):\n#         tsfresh_x_df = tsfresh_x_df.append({'shop_id': shop_id, 'date_block_num': step, 'sale_cnt_month':seq_array[step]}, ignore_index=True)\n#     tsfresh_y_df = tsfresh_y_df.append({'shop_id': shop_id, 'sale_cnt_month':Y_shop[idx][0]}, ignore_index=True)\n# tsfresh_y_df = tsfresh_y_df.set_index('shop_id')['sale_cnt_month']\n\n\n# # generate tsfresh features\n# features_filtered = extract_relevant_features(tsfresh_x_df, tsfresh_y_df,\n#                                               column_id='shop_id', column_sort='date_block_num')\n# print(\"The shape of tsfresh dataframe:\")\n# print(features_filtered.shape)  # (shop_num, feature_num)\n\n\n# # concat the tsfresh back to X_train\n# ## X_train_static\n# tsfresh_feature_list = []\n# for entry in X_train_static:\n#     shop_id = int(entry[0])\n#     tsfresh_feature = features_filtered.loc[[str(shop_id)]].to_numpy()  # (1, tsfresh_num)\n#     tsfresh_feature_list.append(tsfresh_feature)\n# tsfresh_feature_array = np.concatenate(tsfresh_feature_list, axis=0)\n# print(\"The shape of X tsfresh array shape:\")\n# print(tsfresh_feature_array.shape)  # (N, tsfresh_num)\n\n# X_train_static = np.concatenate([X_train_static, tsfresh_feature_array], axis=1)\n# print(\"The shape of X_train_static:\")\n# print(X_train_static.shape)  # (N, static_dim+tsfresh_num)\n\n# ## X_val_static\n# tsfresh_feature_list = []\n# for entry in X_val_static:\n#     shop_id = int(entry[0])\n#     tsfresh_feature = features_filtered.loc[[str(shop_id)]].to_numpy()  # (1, feature_num)\n#     tsfresh_feature_list.append(tsfresh_feature)\n# tsfresh_feature_array = np.concatenate(tsfresh_feature_list, axis=0)\n# print(\"The shape of X tsfresh array shape:\")\n# print(tsfresh_feature_array.shape)  # (N, feature_num)\n\n# X_val_static = np.concatenate([X_val_static, tsfresh_feature_array], axis=1)\n# print(\"The shape of X_val_static:\")\n# print(X_val_static.shape)  # (N, static_dim+tsfresh_num)\n\n\n# ### MODEL ###\n\n# # Model Parameters\n# STEPS = X_train_seq.shape[1]\n# STATIC_LEN = X_train_static.shape[1]\n\n# import os\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# from math import log\n# import tensorflow as tf\n# import tensorflow_probability as tfp\n# from tensorflow import feature_column\n# from tensorflow.nn import relu\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras import Model\n# from tensorflow.keras import Input\n# from tensorflow.keras.layers import Dense, Lambda, Concatenate, Reshape, Embedding, Conv1D, Add, Flatten, LSTM, Activation, BatchNormalization\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.metrics import mean_squared_error\n\n# # Loss\n# def root_mean_squared_error(y_true, y_pred):\n#     return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n# # Model\n# # Wide Component\n# input_static_vec = Input(name='input_static', shape = (STATIC_LEN,))\n\n# shop_vec = Lambda(lambda x: tf.slice(x, (0,0), (-1,1)))(input_static_vec)\n# shop_emb_vec = Embedding(input_dim=60, output_dim=4, input_length=(1,))(shop_vec)\n# shop_emb_vec = Flatten()(shop_emb_vec)\n# item_vec = Lambda(lambda x: tf.slice(x, (0,1), (-1,1)))(input_static_vec)\n# item_emb_vec = Embedding(input_dim=ITEM_VOCAB_LEN, output_dim=4, input_length=(1,))(item_vec)\n# item_emb_vec = Flatten()(item_emb_vec)\n# num_static_vec = Lambda(lambda x: tf.slice(x, (0,2), (-1,STATIC_LEN-2)))(input_static_vec)\n# wide_vec = Concatenate(axis=-1, name='Concat_wide')([shop_emb_vec, item_emb_vec, num_static_vec])\n\n# # Deep Component\n# input_seq_vec = Input(name='input_seq', shape = (STEPS,1))\n\n# deep_vec = LSTM(units=4, name='LSTM')(input_seq_vec)\n\n# # Fusion Layer\n# output_vec = Concatenate(axis=-1, name='Concat_fusion')([wide_vec, deep_vec])\n# output_vec = Dense(units=1, activation='relu')(output_vec)\n\n# model = Model(inputs=[input_static_vec,input_seq_vec], outputs=[output_vec])\n# model.compile(loss=root_mean_squared_error, optimizer=Adam(learning_rate=0.05))\n# model.summary()\n\n# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001,\n#                                               verbose=1, mode='min', patience=3,\n#                                               restore_best_weights=True)]\n\n# history = model.fit(x={'input_static': X_train_static, 'input_seq': X_train_seq}, \n#                     y=Y_train,\n#                     sample_weight=sample_weight,\n#                     validation_data=({'input_static':X_val_static,'input_seq':X_val_seq}, Y_val), \n#                     epochs=100, \n#                     batch_size=1024*1024,\n#                     callbacks=callbacks)\n\n# # tsfresh Model, loss: 0.2499 - val_loss: 0.9606 - test_loss: 4.4129 (V14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Attention Model\n\n# # Model Parameters\n# STEPS = X_train_seq.shape[1]\n\n# import os\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# from math import log\n# import tensorflow as tf\n# import tensorflow_probability as tfp\n# from tensorflow import feature_column\n# from tensorflow.nn import relu\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras import Model\n# from tensorflow.keras import Input\n# from tensorflow.keras.layers import Dense, Lambda, Concatenate, Reshape, Embedding, Conv1D, Add, Flatten, LSTM, Activation, BatchNormalization, Attention\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.metrics import mean_squared_error\n\n# # Loss\n# def root_mean_squared_error(y_true, y_pred):\n#     return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n# # Model\n# # Wide Component\n# input_static_vec = Input(name='input_static', shape = (3,))\n\n# shop_vec = Lambda(lambda x: tf.slice(x, (0,0), (-1,1)))(input_static_vec)\n# shop_emb_vec = Embedding(input_dim=60, output_dim=4, input_length=(1,))(shop_vec)\n# shop_emb_vec = Flatten()(shop_emb_vec)\n# item_vec = Lambda(lambda x: tf.slice(x, (0,1), (-1,1)))(input_static_vec)\n# item_emb_vec = Embedding(input_dim=ITEM_VOCAB_LEN, output_dim=4, input_length=(1,))(item_vec)\n# item_emb_vec = Flatten()(item_emb_vec)\n# price_vec = Lambda(lambda x: tf.slice(x, (0,2), (-1,1)))(input_static_vec)\n# wide_vec = Concatenate(axis=-1, name='Concat_wide')([shop_emb_vec, item_emb_vec, price_vec])\n\n# # Deep Component\n# input_seq_vec = Input(name='input_seq', shape = (STEPS,1))  # (N, steps, 1)\n# # self-attention\n# deep_vec = Attention()([input_seq_vec, input_seq_vec, input_seq_vec], return_attention_scores=False)  # (N, steps, 1)\n# deep_vec = LSTM(units=4, name='LSTM')(deep_vec)\n\n# # Fusion Layer\n# output_vec = Concatenate(axis=-1, name='Concat_fusion')([wide_vec, deep_vec])\n# output_vec = Dense(units=1, activation='relu')(output_vec)\n\n# model = Model(inputs=[input_static_vec,input_seq_vec], outputs=[output_vec])\n# model.compile(loss=root_mean_squared_error, optimizer=Adam(learning_rate=0.05))\n# model.summary()\n\n# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001,\n#                                               verbose=1, mode='min', patience=3,\n#                                               restore_best_weights=True)]\n\n# history = model.fit(x={'input_static': X_train_static, 'input_seq': X_train_seq}, \n#                     y=Y_train,\n#                     sample_weight=sample_weight,\n#                     validation_data=({'input_static':X_val_static,'input_seq':X_val_seq}, Y_val), \n#                     epochs=100, \n#                     batch_size=1024*1024,\n#                     callbacks=callbacks)\n\n# # Attention Model, loss: 0.1141 - val_loss: 0.4449 - test_loss: 6.1804 (V15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # CNN+LSTM Model\n\n# # Model Parameters\n# STEPS = X_train_seq.shape[1]\n\n# import os\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# from math import log\n# import tensorflow as tf\n# import tensorflow_probability as tfp\n# from tensorflow import feature_column\n# from tensorflow.nn import relu\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras import Model\n# from tensorflow.keras import Input\n# from tensorflow.keras.layers import Dense, Lambda, Concatenate, Reshape, Embedding, Conv1D, Add, Flatten, LSTM, Activation, BatchNormalization\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.metrics import mean_squared_error\n\n# # Loss\n# def root_mean_squared_error(y_true, y_pred):\n#     return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n# # Model\n# # Wide Component\n# input_static_vec = Input(name='input_static', shape = (3,))\n\n# shop_vec = Lambda(lambda x: tf.slice(x, (0,0), (-1,1)))(input_static_vec)\n# shop_emb_vec = Embedding(input_dim=60, output_dim=4, input_length=(1,))(shop_vec)\n# shop_emb_vec = Flatten()(shop_emb_vec)\n# item_vec = Lambda(lambda x: tf.slice(x, (0,1), (-1,1)))(input_static_vec)\n# item_emb_vec = Embedding(input_dim=ITEM_VOCAB_LEN, output_dim=4, input_length=(1,))(item_vec)\n# item_emb_vec = Flatten()(item_emb_vec)\n# price_vec = Lambda(lambda x: tf.slice(x, (0,2), (-1,1)))(input_static_vec)\n# wide_vec = Concatenate(axis=-1, name='Concat_wide')([shop_emb_vec, item_emb_vec, price_vec])\n\n# # Deep Component\n# input_seq_vec = Input(name='input_seq', shape = (STEPS,1))\n\n# deep_vec = Conv1D(filters=4, kernel_size=3, strides=1, padding='valid', name='CNN')(input_seq_vec)  # (N,31,4)\n# deep_vec = LSTM(units=4, name='LSTM')(deep_vec)  # (N,4)\n\n# # Fusion Layer\n# output_vec = Concatenate(axis=-1, name='Concat_fusion')([wide_vec, deep_vec])\n# output_vec = Dense(units=1, activation='relu')(output_vec)\n\n# model = Model(inputs=[input_static_vec,input_seq_vec], outputs=[output_vec])\n# model.compile(loss=root_mean_squared_error, optimizer=Adam(learning_rate=0.05))\n# model.summary()\n\n# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001,\n#                                               verbose=1, mode='min', patience=3,\n#                                               restore_best_weights=True)]\n\n# history = model.fit(x={'input_static': X_train_static, 'input_seq': X_train_seq}, \n#                     y=Y_train,\n#                     sample_weight=sample_weight,\n#                     validation_data=({'input_static':X_val_static,'input_seq':X_val_seq}, Y_val), \n#                     epochs=100, \n#                     batch_size=1024*1024,\n#                     callbacks=callbacks)\n\n# # CNN+LSTM Model, - loss: 0.2341 - val_loss: 0.8626 - test_loss: 5.3613 (V16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CNN Model\n\n# Model Parameters\nSTEPS = X_train_seq.shape[1]\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nfrom math import log\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow import feature_column\nfrom tensorflow.nn import relu\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Dense, Lambda, Concatenate, Reshape, Embedding, Conv1D, Add, Flatten, LSTM, Activation, BatchNormalization, AveragePooling1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import mean_squared_error\n\n# Loss\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n# Model\n# Wide Component\ninput_static_vec = Input(name='input_static', shape = (3,))\n\nshop_vec = Lambda(lambda x: tf.slice(x, (0,0), (-1,1)))(input_static_vec)\nshop_emb_vec = Embedding(input_dim=60, output_dim=4, input_length=(1,))(shop_vec)\nshop_emb_vec = Flatten()(shop_emb_vec)\nitem_vec = Lambda(lambda x: tf.slice(x, (0,1), (-1,1)))(input_static_vec)\nitem_emb_vec = Embedding(input_dim=ITEM_VOCAB_LEN, output_dim=4, input_length=(1,))(item_vec)\nitem_emb_vec = Flatten()(item_emb_vec)\nprice_vec = Lambda(lambda x: tf.slice(x, (0,2), (-1,1)))(input_static_vec)\nwide_vec = Concatenate(axis=-1, name='Concat_wide')([shop_emb_vec, item_emb_vec, price_vec])\n\n# Deep Component\ninput_seq_vec = Input(name='input_seq', shape = (STEPS,1))\n\ndeep_vec = Conv1D(filters=4, kernel_size=3, strides=1, padding='valid', name='CNN_1')(input_seq_vec)  # (N,31,4)\ndeep_vec = AveragePooling1D(pool_size=5, strides=5, padding='valid', name='Pooling_1')(deep_vec)  # (N,6,4)\ndeep_vec = Conv1D(filters=4, kernel_size=3, strides=1, padding='valid', name='CNN_2')(deep_vec)  # (N,4,4)\ndeep_vec = AveragePooling1D(pool_size=4, strides=4, padding='valid', name='Pooling_2')(deep_vec)  # (N,1,4)\ndeep_vec = Flatten()(deep_vec)  # (N,4)\n\n\n# Fusion Layer\noutput_vec = Concatenate(axis=-1, name='Concat_fusion')([wide_vec, deep_vec])\noutput_vec = Dense(units=1, activation='relu')(output_vec)\n\nmodel = Model(inputs=[input_static_vec,input_seq_vec], outputs=[output_vec])\nmodel.compile(loss=root_mean_squared_error, optimizer=Adam(learning_rate=0.05))\nmodel.summary()\n\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001,\n                                              verbose=1, mode='min', patience=3,\n                                              restore_best_weights=True)]\n\nhistory = model.fit(x={'input_static': X_train_static, 'input_seq': X_train_seq}, \n                    y=Y_train,\n                    sample_weight=sample_weight,\n                    validation_data=({'input_static':X_val_static,'input_seq':X_val_seq}, Y_val), \n                    epochs=100, \n                    batch_size=1024*1024,\n                    callbacks=callbacks)\n\n# CNN Model, - loss: 0.1605 - val_loss: 0.6452 - test_loss: 4.4129 (V26)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # LSTM Fusion Model\n\n# # Model Parameters\n# STEPS = X_train_seq.shape[1]\n\n# import os\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# from math import log\n# import tensorflow as tf\n# import tensorflow_probability as tfp\n# from tensorflow import feature_column\n# from tensorflow.nn import relu\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras import Model\n# from tensorflow.keras import Input\n# from tensorflow.keras.layers import Dense, Lambda, Concatenate, Reshape, Embedding, Conv1D, Add, Flatten, LSTM, Activation, BatchNormalization, RepeatVector\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.metrics import mean_squared_error\n\n# # Loss\n# def root_mean_squared_error(y_true, y_pred):\n#     return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n# # Model\n# # Wide Component\n# input_static_vec = Input(name='input_static', shape = (3,))\n\n# shop_vec = Lambda(lambda x: tf.slice(x, (0,0), (-1,1)))(input_static_vec)\n# shop_emb_vec = Embedding(input_dim=60, output_dim=4, input_length=(1,))(shop_vec)\n# shop_emb_vec = Flatten()(shop_emb_vec)\n# item_vec = Lambda(lambda x: tf.slice(x, (0,1), (-1,1)))(input_static_vec)\n# item_emb_vec = Embedding(input_dim=ITEM_VOCAB_LEN, output_dim=4, input_length=(1,))(item_vec)\n# item_emb_vec = Flatten()(item_emb_vec)\n# price_vec = Lambda(lambda x: tf.slice(x, (0,2), (-1,1)))(input_static_vec)\n# wide_vec = Concatenate(axis=-1, name='Concat_wide')([shop_emb_vec, item_emb_vec, price_vec])  # (N, dim)\n\n# # Deep Component\n# input_seq_vec = Input(name='input_seq', shape = (STEPS,1))  # (N, steps, 1)\n\n# # Fusion Layer by LSTM\n# ## prepare Fusion input\n# wide_vec = RepeatVector(STEPS)(wide_vec)  # (N, steps, dim)\n# fusion_vec = Concatenate(axis=-1, name='Concat_fusion')([input_seq_vec, wide_vec])  # (N, steps, 1+dim)\n# ## fusion layer\n# output_vec = LSTM(units=4, name='LSTM_fusion')(fusion_vec)  # (N,4)\n# output_vec = Dense(units=1, activation='relu')(output_vec)\n\n# model = Model(inputs=[input_static_vec,input_seq_vec], outputs=[output_vec])\n# model.compile(loss=root_mean_squared_error, optimizer=Adam(learning_rate=0.05))\n# model.summary()\n\n# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001,\n#                                               verbose=1, mode='min', patience=3,\n#                                               restore_best_weights=True)]\n\n# history = model.fit(x={'input_static': X_train_static, 'input_seq': X_train_seq}, \n#                     y=Y_train,\n#                     sample_weight=sample_weight,\n#                     validation_data=({'input_static':X_val_static,'input_seq':X_val_seq}, Y_val), \n#                     epochs=100, \n#                     batch_size=1024*1024,\n#                     callbacks=callbacks)\n\n# # LSTM Fusion Model, - loss: 0.2510 - val_loss: 0.9487 - test_loss: 6.8452 (V17)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initial Model - Bayesian Optimization\n\n# # Model Parameters\n# STEPS = X_train_seq.shape[1]\n\n# import os\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# from math import log\n# import tensorflow as tf\n# import tensorflow_probability as tfp\n# from tensorflow import feature_column\n# from tensorflow.nn import relu\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras import Model\n# from tensorflow.keras import Input\n# from tensorflow.keras.layers import Dense, Lambda, Concatenate, Reshape, Embedding, Conv1D, Add, Flatten, LSTM, Activation, BatchNormalization\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.metrics import mean_squared_error\n# import keras_tuner as kt\n\n# def model_builder(hp):\n#     # Optimized Hyperparameter\n#     hp_emb_size = hp.Choice('emb_size', values=[2,4,8,16])\n#     hp_lstm_units = hp.Choice('lstm_units', values=[1,2,4,8,16])\n#     hp_learning_rate = hp.Float('learning_rate', min_value=0.002, max_value=0.1, step=0.002)\n    \n#     # Loss\n#     def root_mean_squared_error(y_true, y_pred):\n#         return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n#     # Model\n#     # Wide Component\n#     input_static_vec = Input(name='input_static', shape = (3,))\n\n#     shop_vec = Lambda(lambda x: tf.slice(x, (0,0), (-1,1)))(input_static_vec)\n#     shop_emb_vec = Embedding(input_dim=60, output_dim=hp_emb_size, input_length=(1,))(shop_vec)\n#     shop_emb_vec = Flatten()(shop_emb_vec)\n#     item_vec = Lambda(lambda x: tf.slice(x, (0,1), (-1,1)))(input_static_vec)\n#     item_emb_vec = Embedding(input_dim=ITEM_VOCAB_LEN, output_dim=hp_emb_size, input_length=(1,))(item_vec)\n#     item_emb_vec = Flatten()(item_emb_vec)\n#     price_vec = Lambda(lambda x: tf.slice(x, (0,2), (-1,1)))(input_static_vec)\n#     wide_vec = Concatenate(axis=-1, name='Concat_wide')([shop_emb_vec, item_emb_vec, price_vec])\n\n#     # Deep Component\n#     input_seq_vec = Input(name='input_seq', shape = (STEPS,1))\n#     deep_vec = LSTM(units=hp_lstm_units, name='LSTM')(input_seq_vec)\n\n#     # Fusion Layer\n#     output_vec = Concatenate(axis=-1, name='Concat_fusion')([wide_vec, deep_vec])\n#     output_vec = Dense(units=1, activation='relu')(output_vec)\n\n#     model = Model(inputs=[input_static_vec,input_seq_vec], outputs=[output_vec])\n#     model.compile(loss=root_mean_squared_error, optimizer=Adam(learning_rate=hp_learning_rate))\n    \n#     return model\n\n# # Tuner\n# tuner = kt.BayesianOptimization(hypermodel=model_builder,\n#                                 objective='val_loss',\n#                                 max_trials=10,\n#                                 num_initial_points=3,\n#                                 project_name='Initial_Model_Tuner')\n\n# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001,\n#                                               verbose=1, mode='min', patience=3,\n#                                               restore_best_weights=True)]\n\n# tuner.search(x={'input_static': X_train_static, 'input_seq': X_train_seq}, \n#              y=Y_train,\n#              sample_weight=sample_weight,\n#              validation_data=({'input_static':X_val_static,'input_seq':X_val_seq}, Y_val), \n#              epochs=100, \n#              batch_size=1024*1024,\n#              callbacks=callbacks)\n# print('\\n')\n# tuner.search_space_summary()\n# print('\\n')\n# tuner.results_summary()\n\n# # Get the optimal hyperparameters\n# best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n# print(type(best_hps))\n# print(best_hps.values)\n# print(f\"\"\"\n# The best emb_size is {best_hps.get('emb_size')} \n# The best lstm_units is {best_hps.get('lstm_units')}\n# The best learning_rate is {best_hps.get('learning_rate')}\n# \"\"\")\n\n# # Get the optimal model\n# model = tuner.hypermodel.build(best_hps)  # better than use get_best_model, which retrieves the best model in all the trails, not the best hyperparameter after the last trail\n# model.summary()\n# history = model.fit(x={'input_static': X_train_static, 'input_seq': X_train_seq}, \n#                     y=Y_train,\n#                     sample_weight=sample_weight,\n#                     validation_data=({'input_static':X_val_static,'input_seq':X_val_seq}, Y_val), \n#                     epochs=100, \n#                     batch_size=1024*1024,\n#                     callbacks=callbacks)\n\n# # Initial Model (Bayesian Optimization), - loss: 0.1255 - val_loss: 0.5140 - test_loss: 4.4526 (V20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initial Model, Short-term + Long-term\n\n# # Model Parameters\n# STEPS = X_train_seq.shape[1]\n\n# import os\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# from math import log\n# import tensorflow as tf\n# import tensorflow_probability as tfp\n# from tensorflow import feature_column\n# from tensorflow.nn import relu\n# from tensorflow.keras import backend as K\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras import Model\n# from tensorflow.keras import Input\n# from tensorflow.keras.layers import Dense, Lambda, Concatenate, Reshape, Embedding, Conv1D, Add, Flatten, LSTM, Activation, BatchNormalization\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.metrics import mean_squared_error\n\n# # Loss\n# def root_mean_squared_error(y_true, y_pred):\n#     return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n# # Model\n# # Wide Component\n# input_static_vec = Input(name='input_static', shape = (3,))\n\n# shop_vec = Lambda(lambda x: tf.slice(x, (0,0), (-1,1)))(input_static_vec)\n# shop_emb_vec = Embedding(input_dim=60, output_dim=4, input_length=(1,))(shop_vec)\n# shop_emb_vec = Flatten()(shop_emb_vec)\n# item_vec = Lambda(lambda x: tf.slice(x, (0,1), (-1,1)))(input_static_vec)\n# item_emb_vec = Embedding(input_dim=ITEM_VOCAB_LEN, output_dim=4, input_length=(1,))(item_vec)\n# item_emb_vec = Flatten()(item_emb_vec)\n# price_vec = Lambda(lambda x: tf.slice(x, (0,2), (-1,1)))(input_static_vec)\n# wide_vec = Concatenate(axis=-1, name='Concat_wide')([shop_emb_vec, item_emb_vec, price_vec])\n\n# # Deep Component - Long-Term\n# input_seq_vec = Input(name='input_seq', shape = (STEPS,1))\n# deep_long_vec = LSTM(units=4, name='LSTM_long')(input_seq_vec)\n\n# # Deep Component - Short-Term\n# deep_short_vec = Lambda(lambda x: tf.slice(x,(0,STEPS-3,0),(-1,3,-1)), name='short_term_Slice')(input_seq_vec)\n# deep_short_vec = LSTM(units=4, name='LSTM_short')(deep_short_vec)\n\n# # Fusion Layer\n# output_vec = Concatenate(axis=-1, name='Concat_fusion')([wide_vec, deep_short_vec, deep_long_vec])\n# output_vec = Dense(units=1, activation='relu')(output_vec)\n\n# model = Model(inputs=[input_static_vec,input_seq_vec], outputs=[output_vec])\n# model.compile(loss=root_mean_squared_error, optimizer=Adam(learning_rate=0.05))\n# model.summary()\n\n# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001,\n#                                               verbose=1, mode='min', patience=3,\n#                                               restore_best_weights=True)]\n\n# history = model.fit(x={'input_static': X_train_static, 'input_seq': X_train_seq}, \n#                     y=Y_train,\n#                     sample_weight=sample_weight,\n#                     validation_data=({'input_static':X_val_static,'input_seq':X_val_seq}, Y_val), \n#                     epochs=100, \n#                     batch_size=1024*1024,\n#                     callbacks=callbacks)\n\n# # Initial Model (Short-term + Long-term), loss: 0.2261 - val_loss: 0.8831 - test_loss: 4.6863 (V22)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, to_file='model_graph.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['val_loss'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare Test dataset\n\ntest_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\ntest_df['shop_item'] = test_df['shop_id'].astype('string') + '-' + test_df['item_id'].astype('string')\n\nX_test_static = []\nX_test_seq = []\nfor _, row in test_df.iterrows():\n    shop_item = row['shop_item']\n    shop_id = str(row['shop_id'])\n    item_id = str(row['item_id'])\n    \n    # get sale_cnt_seq\n    if shop_item in data_norm_seq_dict.keys():\n        sale_norm_cnt_seq = data_norm_seq_dict[shop_item][1:]\n    else:\n        if item_id in item_norm_seq_dict.keys():\n            sale_norm_cnt_seq = item_norm_seq_dict[item_id][1:]\n        else:\n            sale_norm_cnt_seq = [0.0 for i in range(0,STEPS)]\n    \n    # get price\n    if shop_item in shopitem_price_dict.keys():\n        price = shopitem_price_dict[shop_item]\n    else:\n        if item_id in item_price_dict.keys():\n            price = item_price_dict[item_id]\n        else:\n            price = 0.0\n    \n    # generate the sample\n    static_vec = np.array([int(shop_id), int(item_id) % ITEM_VOCAB_LEN])\n    price_vec = price_scaler.transform([[price]]).reshape((-1,))\n    static_vec = np.concatenate([static_vec, price_vec])\n    seq_vec = np.array(sale_norm_cnt_seq).reshape((-1,1))  # (steps, 1)\n    X_test_static.append(static_vec)\n    X_test_seq.append(seq_vec)\nX_test_static = np.array(X_test_static)\nX_test_seq = np.array(X_test_seq)\n\n# ## [tsfresh Model] ##\n# # concat tsfresh feature to X_test_static\n# tsfresh_feature_list = []\n# for entry in X_test_static:\n#     shop_id = int(entry[0])\n#     tsfresh_feature = features_filtered.loc[[str(shop_id)]].to_numpy()  # (1, tsfresh_num)\n#     tsfresh_feature_list.append(tsfresh_feature)\n# tsfresh_feature_array = np.concatenate(tsfresh_feature_list, axis=0)\n# print(\"The shape of X_test tsfresh array shape:\")\n# print(tsfresh_feature_array.shape)  # (N, tsfresh_num)\n\n# X_test_static = np.concatenate([X_test_static, tsfresh_feature_array], axis=1)\n# ## [tsfresh Model] ##\n\n\nprint('X_test_static:')\nprint(X_test_static.shape)\nprint(X_test_static)\nprint('\\nX_test_seq:')\nprint(X_test_seq.shape)\nprint(X_test_seq)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict({'input_static':X_test_static, 'input_seq':X_test_seq})\nprint(pred.shape)\nprint(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recover the prediction (de-normalized)\nresults = []\nfor i in range(0,pred.shape[0]):\n    shop_id = str(test_df.iloc[i]['shop_id'])\n    item_id = str(test_df.iloc[i]['item_id'])\n    shop_item = shop_id + '-' + item_id\n    if shop_item in data_norm_seq_dict.keys():\n        mean = data_norm_seq_mean_dict[shop_item]\n        stddev = data_norm_seq_stddev_dict[shop_item]\n        result = pred[i][0] * stddev + mean\n    else:\n        if item_id in item_norm_seq_dict.keys():\n            mean = item_norm_seq_mean_dict[item_id]\n            stddev = item_norm_seq_stddev_dict[item_id]\n            result = pred[i][0] * stddev + mean\n        else:\n            result = 0.0\n    if result < 0.0:\n        result = 0.0\n    results.append(result)\nresults[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(results, columns=['item_cnt_month'])\nsubmission_df.index.name = 'ID'\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('/kaggle/working/submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Investigate the importance of \"yesterday Wednesday\" and \"last Thurseday\"\n# data_seq_array = np.array(list(data_seq_dict.values()))  # (N, 34)\n# Conclusion: RMSE_last_day = 3.47, RMSE_last_Sunday = 3.83, \n# the \"recent time\" is more important, however it is not significant.\n\n# # rmse of last step\n# rmse_last_step = np.sqrt(np.mean((data_seq_array[:,-7:] - data_seq_array[:,-8:-1])**2))\n# print(rmse_last_step)\n\n# # rmse of the corresponding step in last period\n# rmse_crspd_last_perid = np.sqrt(np.mean((data_seq_array[:,-7:] - data_seq_array[:,-14:-7])**2))\n# print(rmse_crspd_last_perid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}