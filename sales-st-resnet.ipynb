{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"For **ST-ResNet**, it leverages sequence data (sale_cnt) and external factors (shop_id, item_id, price).\n* input 1: [sale_cnt seq], shape = (N,step)\n* input 2: [shop_id, item_id, price], shape = (N,3)\n\nThis is a version of ST-ResNet applied on 1D Time-Series Prediction, so the model structure is simplified.","metadata":{}},{"cell_type":"code","source":"# Read data and aggregate into monthly sale_cnt and avg_price of shop_item\n\nimport pandas as pd\n\ntrain_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\n# columns: date, date_block_num, shop_id, item_id, item_price, item_cnt_day\ntrain_monthly_df = train_df.groupby(['date_block_num','shop_id','item_id']).agg(sale_cnt_month=('item_cnt_day','sum'), avg_price_month=('item_price','mean')).reset_index()\ntrain_monthly_df['shop_item'] = train_monthly_df['shop_id'].astype('string') + '-' + train_monthly_df['item_id'].astype('string')\nshop_item_list = pd.unique(train_monthly_df['shop_item'])\nshop_id_list = pd.unique(train_monthly_df['shop_id'])\nitem_id_list = pd.unique(train_monthly_df['item_id'])\n\nprint(\"count of shop_item:\",len(shop_item_list))\nprint('------')\nprint('count of shop_id:', len(shop_id_list))\nprint('min shop_id:', min(shop_id_list))\nprint('max shop id:', max(shop_id_list))\nprint('------')\nprint('count item_id:', len(item_id_list))\nprint('min item_id:', min(item_id_list))\nprint('max item id:', max(item_id_list))\ntrain_monthly_df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the \"sale_cnt sequences\" of each \"shop_item\"\n## get (data_seq_dict) and (data_seq_array)\n\nimport numpy as np\n\ndata_seq_dict = {}\nfor shop_item in shop_item_list:\n    shop_id = float(shop_item.split('-')[0])\n    item_id = float(shop_item.split('-')[1])\n    data_seq_dict[shop_item] = [0.0 for i in range(0,34)]\nfor _, row in train_monthly_df.iterrows():\n    data_seq_dict[row['shop_item']][int(row['date_block_num'])] = row['sale_cnt_month']\n\ndata_seq_array = np.array(list(data_seq_dict.values()))\nprint(data_seq_array.shape)  # (424124, 34)\nprint(data_seq_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the \"last price\" of each \"shop_item\" as the future price\n## shopitem_price_dict\n\nshopitem_price_dict = {}\n# record price and max date_block_num of each shop_item\nfor _, row in train_monthly_df.iterrows():\n    if row['shop_item'] not in shopitem_price_dict.keys():\n        shopitem_price_dict[row['shop_item']] = [row['avg_price_month'], row['date_block_num']]\n    elif row['date_block_num'] > shopitem_price_dict[row['shop_item']][1]:\n        shopitem_price_dict[row['shop_item']] = [row['avg_price_month'], row['date_block_num']]\n# remove date_block_num in the dict to only keep [shop_item, avg_price_month]\nfor k,v in shopitem_price_dict.items():\n    shopitem_price_dict[k] = v[0]\n\nprint(len(shopitem_price_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the avg \"sale_cnt sequences\" of each \"item\" as default\n# In case the shop_item in test dataset does not appear in training dataset, \n# we use the \"item avg sale_cnt sequence\" as default.\n## get (item_seq_dict)\n\nitem_seq_dict = {}\nitem_seq_num_dict = {}\n# make sum\nfor k, v in data_seq_dict.items():\n    item = k.split('-')[1]\n    if item not in item_seq_dict.keys():\n        item_seq_dict[item] = v\n        item_seq_num_dict[item] = 1\n    else:\n        item_seq_dict[item] = [i+j for i, j in zip(item_seq_dict[item], v)]\n        item_seq_num_dict[item] += 1\n# make average\nfor item in item_seq_dict.keys():\n    item_seq_dict[item] = [element / item_seq_num_dict[item] for element in item_seq_dict[item]]\n\nprint(np.array(list(item_seq_dict.values())).shape)  # (num_item, steps)=(21807, 34)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the \"last price\" of each \"item\" as the future price\n## item_price_dict\n\nitem_price_dict = {}  # {item_id: [avg_price, current_max_date_block_num, cnt]}\n# record prices and max date_block_num of each item\nfor _, row in train_monthly_df.iterrows():\n    if row['item_id'] not in item_price_dict.keys():\n        item_price_dict[row['item_id']] = [row['avg_price_month'], row['date_block_num'], 1]\n    else:\n        if row['date_block_num'] > item_price_dict[row['item_id']][1]:\n            item_price_dict[row['item_id']] = [row['avg_price_month'], row['date_block_num'], 1]\n        if row['date_block_num'] == item_price_dict[row['item_id']][1]:\n            cnt = item_price_dict[row['item_id']][2] + 1\n            price = (item_price_dict[row['item_id']][0] * item_price_dict[row['item_id']][2] + row['avg_price_month']) / cnt\n            item_price_dict[row['item_id']] = [price, row['date_block_num'], cnt]\nfor k,v in item_price_dict.items():\n    item_price_dict[k] = v[0]\n\nprint(len(item_price_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot curve\n\nimport matplotlib.pyplot as plt\n\nrow_random_array = np.arange(data_seq_array.shape[0])\nnp.random.shuffle(row_random_array)\nrow_random = data_seq_array[row_random_array][0:100,:]\nfor seq in row_random:\n    plt.plot(seq)\nplt.xlabel('time step')\nplt.ylabel('sale cnt')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import backend as K\n\n# parameter\nITEM_VOCAB_LEN = 500\n\n# OneHotEncoder of feature \nfeature_encoder = OneHotEncoder(handle_unknown='ignore')\nshop_item_mod = np.array(train_monthly_df[['shop_id', 'item_id']])\nshop_item_mod[:,1] = shop_item_mod[:,1] % ITEM_VOCAB_LEN\nfeature_encoder.fit(shop_item_mod)\n# print(one_hot_encoder.categories_)\n\n# Scaler of price\nprice_scaler = MinMaxScaler(feature_range=(0,1))\nprice_scaler.fit([[i] for i in train_monthly_df['avg_price_month'].tolist()])\n\n# Scaler of sale_cnt_seq, feature_range=(-1,1)\nmax_sale_cnt = max(train_monthly_df['avg_price_month'])\nmin_sale_cnt = min(train_monthly_df['avg_price_month'])\nclass SaleCntScaler():\n    def __init__(self):\n        self.fearure_range_min = -1\n        self.fearure_range_max = 1\n    def scale(self, x):\n        return (x - min_sale_cnt) / (max_sale_cnt - min_sale_cnt) * (self.fearure_range_max - self.fearure_range_min) + self.fearure_range_min\n    def descale(self, x):\n        return (x - self.fearure_range_min) / (self.fearure_range_max - self.fearure_range_min) * (max_sale_cnt - min_sale_cnt) + min_sale_cnt\nsale_cnt_scaler = SaleCntScaler()\n\n\n# Split X and Y\nX = []\nY = []\nfor k,v in data_seq_dict.items():\n    shop_vec = int(k.split('-')[0])\n    item_vec = int(k.split('-')[1]) % ITEM_VOCAB_LEN\n    static_vec = feature_encoder.transform([[shop_vec, item_vec]]).toarray().reshape((-1,))\n    price_vec = price_scaler.transform([[shopitem_price_dict[k]]]).reshape((-1,))\n    ext_vec = np.concatenate([static_vec, price_vec])\n    sale_cnt_seq = np.array([sale_cnt_scaler.scale(i) for i in v])\n    X.append([ext_vec, sale_cnt_seq[:-1]])\n    Y.append(sale_cnt_seq[-1])\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1)\nX_train_ext = np.array([x[0] for x in X_train])\nX_train_seq = np.array([x[1] for x in X_train])\nY_train = np.array(Y_train)\nX_val_ext = np.array([x[0] for x in X_val])\nX_val_seq = np.array([x[1] for x in X_val])\nY_val = np.array(Y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Parameters\nfrom math import floor\n\nSTEPS = len(X_train_seq[0])\nEXT_DIM = len(X_train_ext[0])\nCLOSENESS_LEN = 3\nPERIOD_LEN = floor((STEPS - CLOSENESS_LEN)/12)*12\nTREND_LEN = floor((STEPS - CLOSENESS_LEN)/12)*12\nRES_UNIT_NUM = 4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ST-ResNet\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nfrom math import log\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow import feature_column\nfrom tensorflow.nn import relu\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Dense, Lambda, Concatenate, Reshape, Embedding, Conv1D, Add, Flatten, LSTM, Activation, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import mean_squared_error\n\nclass ResUnit(tf.keras.layers.Layer):\n    def __init__(self,name=None):\n        super(ResUnit, self).__init__(name=name)\n        self.bn1 = BatchNormalization()\n        self.bn2 = BatchNormalization()\n        self.relu = Activation('relu')\n        self.conv1 = Conv1D(filters=4,kernel_size=1,strides=1,padding='same')\n        self.conv2 = Conv1D(filters=4,kernel_size=1,strides=1,padding='same')\n        self.add = Add()\n    def call(self, input):\n        output = self.bn1(input)\n        output = self.relu(output)\n        output = self.conv1(output)\n        output = self.bn2(output)\n        output = self.relu(output)\n        output = self.conv2(output)\n        output = self.add([input, output])\n        return output\n\nclass HadamardProductLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(HadamardProductLayer, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel', \n                                      shape=input_shape[1:],  # omit the batch_size dimension\n                                      trainable=True)\n    def call(self, inputs):\n        return tf.multiply(inputs, self.kernel)\n    \n    \n\n# model\ninput_seq_vec = Input(name='input_seq', shape = (STEPS,))  # (N, steps）\n\n# Closeness\n## closeness data preparation\ncloseness_input_vec = Lambda(lambda x: tf.slice(x,(0,STEPS-CLOSENESS_LEN),(-1,CLOSENESS_LEN)))(input_seq_vec)  # (N, closeness_len)\ncloseness_input_vec = Reshape((1,-1))(closeness_input_vec)  # (N, 1, closeness_len), analog with (N, Width*Height, Channel) in CNN\n## closeness component\ncloseness_vec = Conv1D(filters=4,kernel_size=1,strides=1,padding='same',activation='relu',name='closeness_conv1')(closeness_input_vec)  # (N, 1, filters)\nfor i in range(0,RES_UNIT_NUM):\n    closeness_vec = ResUnit()(closeness_vec)\ncloseness_vec = Conv1D(filters=1,kernel_size=1,strides=1,padding='same',activation='relu',name='closeness_conv2')(closeness_vec)  # (N, 1, filters)\n\n# Period\n## period data preparation\nperiod_input_vec = Lambda(lambda x: tf.slice(x,(0,STEPS-PERIOD_LEN-CLOSENESS_LEN),(-1,PERIOD_LEN)))(input_seq_vec)  # (N, period_len)\nperiod_input_vec = Reshape((1,-1))(period_input_vec)  # (N, 1, period_len), analog with (N, Width*Height, Channel) in CNN\n## period component\nperiod_vec = Conv1D(filters=4,kernel_size=1,strides=1,padding='same',activation='relu',name='period_conv1')(period_input_vec)  # (N, 1, filters)\nfor i in range(0,RES_UNIT_NUM):\n    period_vec = ResUnit()(period_vec)\nperiod_vec = Conv1D(filters=1,kernel_size=1,strides=1,padding='same',activation='relu',name='period_conv2')(period_vec)  # (N, 1, filters)\n\n# Trend\n## trend data preparation\ntrend_input_vec = Lambda(lambda x: tf.slice(x,(0,0),(-1,TREND_LEN)))(input_seq_vec)  # (N, trend_len)\ntrend_input_vec = Reshape((1,-1))(trend_input_vec)  # (N, 1, trend_len), analog with (N, Width*Height, Channel) in CNN\n## trend component\ntrend_vec = Conv1D(filters=4,kernel_size=1,strides=1,padding='same',activation='relu',name='trend_conv1')(trend_input_vec)  # (N, 1, filters)\nfor i in range(0,RES_UNIT_NUM):\n    trend_vec = ResUnit()(trend_vec)\ntrend_vec = Conv1D(filters=1,kernel_size=1,strides=1,padding='same',activation='relu',name='trend_conv2')(trend_vec)  # (N, 1, filters)\n\n# Parametric-matrix-based Fusion\ncloseness_vec = HadamardProductLayer()(closeness_vec)\nperiod_vec = HadamardProductLayer()(period_vec)\ntrend_vec = HadamardProductLayer()(trend_vec)\nres_vec = Add()([closeness_vec, period_vec, trend_vec])\nres_vec = Flatten()(res_vec)\n\n# External factors\ninput_ext_vec = Input(name='input_ext', shape = (EXT_DIM,))  # (N, ext_dim)\next_vec = Dense(10, activation='relu')(input_ext_vec)\next_vec = Dense(1, activation='relu')(ext_vec)\n\n# Fusion with external factors\noutput_vec = Add()([res_vec, ext_vec])\noutput_vec = Activation('tanh')(output_vec)\n\nmodel = Model(inputs=[input_seq_vec,input_ext_vec], outputs=[output_vec])\nmodel.compile(loss=mean_squared_error, optimizer=Adam(learning_rate=0.01))\nmodel.summary()\n\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01,\n                                              verbose=1, mode='min', patience=3,\n                                              restore_best_weights=True)]\n\nhistory = model.fit(x={'input_seq': X_train_seq, 'input_ext': X_train_ext}, \n                    y=Y_train, \n                    validation_data=({'input_seq':X_val_seq,'input_ext':X_val_ext}, Y_val), \n                    epochs=20, \n                    batch_size=1024*1024,\n                    callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, to_file='model_graph.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['val_loss'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare Test dataset\n\ntest_df = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\ntest_df['shop_item'] = test_df['shop_id'].astype('string') + '-' + test_df['item_id'].astype('string')\n\nX_test = []\nfor _, row in test_df.iterrows():\n    shop_item = row['shop_item']\n    shop_id = row['shop_id']\n    item_id = row['item_id']\n    \n    # get sale_cnt_seq\n    if shop_item in data_seq_dict.keys():\n        sale_cnt_seq = np.array(data_seq_dict[shop_item])[1:]\n    elif item_id in item_seq_dict.keys():\n        sale_cnt_seq = np.array(item_seq_dict[item_id])[1:]\n        print('For sale_cnt_seq, this shop-item', shop_item, 'was not observed in training samples and use item_id default instead.')\n    else:\n        sale_cnt_seq = np.array([0.0 for i in range(0,STEPS)])\n        print('For sale_cnt_seq, this shop-item', shop_item, 'was not observed in training samples (new item in shop).')\n    \n    # get price\n    if shop_item in shopitem_price_dict.keys():\n        price = shopitem_price_dict[shop_item]\n    elif item_id in item_price_dict.keys():\n        price = item_price_dict[item_id]\n        print('For price, this shop-item', shop_item, 'was not observed in training samples and use item_id default instead.')\n    else:\n        price = 0.0\n        print('For price, this item_id', item_id, 'was not observed in training samples (new item in shop).')\n    \n    # generate the sample\n    shop_vec = int(shop_id)\n    item_vec = int(item_id) % ITEM_VOCAB_LEN\n    static_vec = feature_encoder.transform([[shop_vec, item_vec]]).toarray().reshape((-1,))\n    price_vec = price_scaler.transform([[price]]).reshape((-1,))\n    ext_vec = np.concatenate([static_vec, price_vec])\n    sale_cnt_seq = np.array([sale_cnt_scaler.scale(i) for i in sale_cnt_seq])\n    X_test.append([ext_vec, sale_cnt_seq])\n\nX_test_ext = np.array([x[0] for x in X_test])\nX_test_seq = np.array([x[1] for x in X_test])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict({'input_ext':X_test_ext, 'input_seq':X_test_seq})\nprint(pred[0:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = np.squeeze(pred)\nprint(result.shape)\nprint(min(result))\nprint(max(result))\nprint(result[0:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recover the prediction (de-normalized)\ndescale_func = np.vectorize(sale_cnt_scaler.descale)\nresult = descale_func(result)\nresult[result<0] = 0.0\nprint(min(result))\nprint(max(result))\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(result, columns=['item_cnt_month'])\nsubmission_df.index.name = 'ID'\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('/kaggle/working/submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}